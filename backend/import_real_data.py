import os
import sys
import pandas as pd
import random
from sqlalchemy.orm import Session
from database import SessionLocal, engine, Base
import models
from datetime import datetime

# Setup absolute path to import models and database
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

BASE_DIR = r"c:\Users\kang\Desktop\ë””ìì¸ì§„í¥ì› v1\busan_data"
FILE_PUBLIC = "2ì°¨ì§„ë‹¨_ì¼ë°˜ì¸_251210.xlsx"
FILE_EXPERT = "2ì°¨ì§„ë‹¨_ì „ë¬¸ê°€_251210.xlsx"

def import_data():
    print("Initializing Database...")
    # Drop all tables to ensure schema update
    Base.metadata.drop_all(bind=engine)
    Base.metadata.create_all(bind=engine)
    db = SessionLocal()

    try:
        # Clear existing data (Redundant if dropped, but safe)
        print("Clearing existing data...")
        db.query(models.Persona).delete()
        db.query(models.DistrictInsight).delete()
        db.query(models.DistrictAnalysis).delete()
        db.commit()

        # Load Excel Files
        print("Loading Excel files...")
        public_path = os.path.join(BASE_DIR, FILE_PUBLIC)
        if not os.path.exists(public_path):
             print(f"File not found: {public_path}")
             return

        df_public = pd.read_excel(public_path)
        print(f"Loaded {len(df_public)} rows from {FILE_PUBLIC}")

        # Complete List of Busan Districts
        BUSAN_DISTRICTS = [
            {"id": "21010", "name": "ì¤‘êµ¬"}, {"id": "21020", "name": "ì„œêµ¬"}, {"id": "21030", "name": "ë™êµ¬"},
            {"id": "21040", "name": "ì˜ë„êµ¬"}, {"id": "21050", "name": "ë¶€ì‚°ì§„êµ¬"}, {"id": "21060", "name": "ë™ë˜êµ¬"},
            {"id": "21070", "name": "ë‚¨êµ¬"}, {"id": "21080", "name": "ë¶êµ¬"}, {"id": "21090", "name": "í•´ìš´ëŒ€êµ¬"},
            {"id": "21100", "name": "ì‚¬í•˜êµ¬"}, {"id": "21110", "name": "ê¸ˆì •êµ¬"}, {"id": "21120", "name": "ê°•ì„œêµ¬"},
            {"id": "21130", "name": "ì—°ì œêµ¬"}, {"id": "21140", "name": "ìˆ˜ì˜êµ¬"}, {"id": "21150", "name": "ì‚¬ìƒêµ¬"},
            {"id": "21310", "name": "ê¸°ì¥êµ°"}
        ]
        
        # Mapping from common names/places to District ID
        DISTRICT_MAPPING = {d['name']: d['id'] for d in BUSAN_DISTRICTS}
        DISTRICT_MAPPING.update({
            "ë¶€ì‚°ì—­": "21030", "ì´ˆëŸ‰": "21030",
            "ì„œë©´": "21050", "ì „í¬": "21050", "ë¶€ì „": "21050",
            "ê´‘ì•ˆë¦¬": "21140", "ìˆ˜ì˜": "21140",
            "í•´ìš´ëŒ€": "21090", "ì„¼í…€": "21090",
            "ìê°ˆì¹˜": "21010", "ë‚¨í¬": "21010",
            "ì‚¬ìƒ": "21150", "ë•ì²œ": "21080"
        })

        def get_district_code(raw_name):
            if pd.isna(raw_name): return "21050" 
            s = str(raw_name).strip()
            if s in DISTRICT_MAPPING: return DISTRICT_MAPPING[s]
            for name, code in DISTRICT_MAPPING.items():
                if name in s: return code
            return "21050" 

        def correct_district_by_coords(code, lat, lng):
            if pd.isna(lat) or pd.isna(lng): return code
            # If mapped to Dong-gu (21030) but Lat > 35.155 (Deep in Seomyeon), move to Busanjin-gu
            # Beomil is around 35.14~35.15, keeping it in Dong-gu (21030) if possible
            if code == '21030' and lat > 35.155:
                return '21050'
            return code

        # ID example: 'NYJ76300'
        # We need distinct list of users to create Personas
        print("Processing Personas...")
        
        # Group by ID
        users = df_public.groupby('ID').agg({
            'ì§„ë‹¨ì§€ì—­': 'first',
            'ë¦¬ë·°': lambda x: list(x.dropna()),
            'ì ìˆ˜': 'mean',
            'ìœ„ë„': 'mean',
            'ê²½ë„': 'mean'
        }).reset_index()

        personas = []
        for idx, row in users.iterrows():
            # Generate Persona Attributes
            raw_code = get_district_code(row['ì§„ë‹¨ì§€ì—­'])
            # Apply Spatial Correction
            district_code = correct_district_by_coords(raw_code, row['ìœ„ë„'], row['ê²½ë„'])
            
            avg_score = row['ì ìˆ˜']
            pain_points = row['ë¦¬ë·°'][:5] # Top 5 reviews
            
            # Synthetic attributes
            age = random.choice([20, 30, 40, 50, 60])
            jobs = ['ì§ì¥ì¸', 'í•™ìƒ', 'ì£¼ë¶€', 'ìì˜ì—…ì', 'í”„ë¦¬ëœì„œ']
            job = random.choice(jobs)
            name = f"ì‹œë¯¼ {row['ID'][-4:]}" # Anonymized Name

            persona = models.Persona(
                name=name,
                age=age,
                job=job,
                district_code=str(district_code),
                year="2026", # Target Year
                image_emoji=random.choice(['ğŸ‘©', 'ğŸ‘¨', 'ğŸ§‘', 'ğŸ‘µ', 'ğŸ‘´']),
                tags=["ì•ˆì „", "ë³´í–‰í™˜ê²½", "ì¡°ëª…"], # Default tags
                quote=pain_points[0] if pain_points else "ì•ˆì „í•œ ë¶€ì‚°ì„ ì›í•©ë‹ˆë‹¤.",
                full_quote=f"ì €ëŠ” {district_code}ì— ì‚´ê³  ìˆëŠ”ë°, {pain_points[0] if pain_points else 'ë¶ˆí¸í•œ ì ì´ ë§ìŠµë‹ˆë‹¤.'}",
                pain_points=pain_points,
                suggestions=["ì¡°ëª… ê°œì„ ", "ë³´ë„ ë¸”ë¡ ì •ë¹„", "CCTV ì„¤ì¹˜"],
                expected_effects=["ë³´í–‰ ì•ˆì „ í™•ë³´", "ë²”ì£„ ì˜ˆë°©", "ë„ì‹œ ë¯¸ê´€ ê°œì„ "],
                stats={"safety_score": int(avg_score * 20), "satisfaction": int(avg_score * 20)}
            )
            db.add(persona)
        
        print(f"Created {len(users)} personas.")

        # Complete List of Busan Districts
        BUSAN_DISTRICTS = [
            {"id": "21010", "name": "ì¤‘êµ¬"}, {"id": "21020", "name": "ì„œêµ¬"}, {"id": "21030", "name": "ë™êµ¬"},
            {"id": "21040", "name": "ì˜ë„êµ¬"}, {"id": "21050", "name": "ë¶€ì‚°ì§„êµ¬"}, {"id": "21060", "name": "ë™ë˜êµ¬"},
            {"id": "21070", "name": "ë‚¨êµ¬"}, {"id": "21080", "name": "ë¶êµ¬"}, {"id": "21090", "name": "í•´ìš´ëŒ€êµ¬"},
            {"id": "21100", "name": "ì‚¬í•˜êµ¬"}, {"id": "21110", "name": "ê¸ˆì •êµ¬"}, {"id": "21120", "name": "ê°•ì„œêµ¬"},
            {"id": "21130", "name": "ì—°ì œêµ¬"}, {"id": "21140", "name": "ìˆ˜ì˜êµ¬"}, {"id": "21150", "name": "ì‚¬ìƒêµ¬"},
            {"id": "21310", "name": "ê¸°ì¥êµ°"}
        ]
        
        # Mapping from common names/places to District ID
        DISTRICT_MAPPING = {d['name']: d['id'] for d in BUSAN_DISTRICTS}
        # Add special cases found in data
        DISTRICT_MAPPING.update({
            "ë¶€ì‚°ì—­": "21030", # Dong-gu
            "ì´ˆëŸ‰": "21030",
            "ì„œë©´": "21050", # Busanjin-gu
            "ì „í¬": "21050",
            "ë¶€ì „": "21050",
            "ê´‘ì•ˆë¦¬": "21140", # Suyeong-gu
            "í•´ìš´ëŒ€": "21090", # Haeundae-gu
            "ì„¼í…€": "21090",
            "ìê°ˆì¹˜": "21010", # Jung-gu
            "ë‚¨í¬": "21010",
            "ì‚¬ìƒ": "21150", # Sasang-gu
            "ë•ì²œ": "21080", # Buk-gu
        })

        def get_district_code(raw_name):
            if pd.isna(raw_name): return "21050" # Default?
            s = str(raw_name).strip()
            # Direct match
            if s in DISTRICT_MAPPING: return DISTRICT_MAPPING[s]
            # Partial match (e.g. "ë¶€ì‚°ì§„êµ¬ ë¶€ì „ë™")
            for name, code in DISTRICT_MAPPING.items():
                if name in s:
                    return code
            return "21050" # Default to Busanjin-gu (Center) or known valid if unknown

        def correct_district_by_coords(code, lat, lng):
            # If mapped to Dong-gu (21030) but Lat > 35.14, it's likely Busanjin-gu (Seomyeon)
            # Busan Station: ~35.115, Seomyeon: ~35.157
            if code == '21030' and lat > 35.14:
                return '21050'
            # Add more spatial rules if needed
            return code

        # Helper function to process insights from dataframe
        def process_insights(df, source_type):
            count = 0
            
            for idx, row in df.iterrows():
                # Skip if no coordinates
                if pd.isna(row['ìœ„ë„']) or pd.isna(row['ê²½ë„']):
                    continue
                    
                # Determine Severity based on Score (Numeric or String)
                score_val = row['ì ìˆ˜']
                is_high_severity = False
                
                if isinstance(score_val, (int, float)):
                    if score_val <= 2:
                        is_high_severity = True
                elif isinstance(score_val, str):
                    if "ë¶€ì í•©" in score_val:
                        is_high_severity = True
                
                # Correctly map the district code
                raw_district = row['ì§„ë‹¨ì§€ì—­']
                mapped_code = get_district_code(raw_district)
                # Apply Spatial Correction
                final_code = correct_district_by_coords(mapped_code, float(row['ìœ„ë„']), float(row['ê²½ë„']))

                insight = models.DistrictInsight(
                    district_code=final_code,
                    year="2026", # Target Year
                    type='issue', # Default
                    title=f"{row['ëŒ€ë¶„ë¥˜']} - {row['ì¤‘ë¶„ë¥˜']} ë¬¸ì œ",
                    description=row['ë¦¬ë·°'] if pd.notna(row['ë¦¬ë·°']) else "ë‚´ìš© ì—†ìŒ",
                    image_url=row['ì´ë¯¸ì§€ê²½ë¡œ'] if 'ì´ë¯¸ì§€ê²½ë¡œ' in row and pd.notna(row['ì´ë¯¸ì§€ê²½ë¡œ']) else None,
                    severity='High' if is_high_severity else 'Medium',
                    date=str(row['ë“±ë¡ì¼ì‹œ']) if 'ë“±ë¡ì¼ì‹œ' in row else str(datetime.now()),
                    proposer=f"{'ì „ë¬¸ê°€' if source_type == 'diagnosis' else 'ì‹œë¯¼'} {str(row['ID'])[-4:]}",
                    latitude=float(row['ìœ„ë„']),
                    longitude=float(row['ê²½ë„']),
                    category=source_type # 'survey' or 'diagnosis'
                )
                db.add(insight)
                count += 1
            return count

        # Process Public Insights (Survey)
        print("Processing Public Insights...")
        df_public_insights = df_public.dropna(subset=['ë¦¬ë·°', 'ì´ë¯¸ì§€ê²½ë¡œ'])
        public_count = process_insights(df_public_insights, 'survey')
        print(f"Created {public_count} public insights.")

        # Load and Process Expert Insights (Diagnosis)
        print("Loading Expert Excel file...")
        expert_path = os.path.join(BASE_DIR, FILE_EXPERT)
        if os.path.exists(expert_path):
            df_expert = pd.read_excel(expert_path)
            print(f"Loaded {len(df_expert)} rows from {FILE_EXPERT}")
            expert_count = process_insights(df_expert, 'diagnosis')
            print(f"Created {expert_count} expert insights.")
        else:
            print(f"Expert file not found: {expert_path}")

        analysis_data = {}
        # Real data processing (as before)
        district_scores = df_public.groupby(['ì§„ë‹¨ì§€ì—­', 'ëŒ€ë¶„ë¥˜'])['ì ìˆ˜'].mean().reset_index()
        
        # Simple Mapping Strategy
        category_map = {
            'ë³´ë„ (ê³µê³µê³µê°„)': 'transport_score',
            'ìœ„ìƒì‹œì„¤': 'env_score',
            'ê³µê³µê³µê°„': 'culture_score',
            'ì•ˆë‚´ì‹œì„¤': 'welfare_score',
            # Add more if found
        }

        # Mapping for Real Data
        for idx, row in district_scores.iterrows():
            d_code = str(row['ì§„ë‹¨ì§€ì—­'])
            # Basic mapping if d_code matches district name, optional
            # But the real data has 'ë¶€ì‚°ì—­' which is not a district code. 
            # We map 'ë¶€ì‚°ì—­' to '21030' (Dong-gu) for the sake of the dashboard
            if d_code == 'ë¶€ì‚°ì—­': 
                d_code = '21030'
            elif d_code == 'ê´‘ì•ˆë¦¬':
                d_code = '21140'
            
            cat = row['ëŒ€ë¶„ë¥˜']
            score_100 = (row['ì ìˆ˜'] / 5.0) * 100
            
            if d_code not in analysis_data:
                analysis_data[d_code] = {}
            if cat in category_map:
                analysis_data[d_code][category_map[cat]] = score_100

        # Augment with Dummy Data for ALL Districts and Years
        YEARS = ['2026', '2025', '2024']
        
        # Approximate centers for map jittering (Lat, Lng) - simplified as offsets from a base valid point or just range
        # Since we don't have exact shapes, we'll just use the expert data points as seeds if available, 
        # or fall back to a rough Busan bounding box jittered. 
        # Better: distinct base coordinates for each district hardcoded to avoid "sea" points.
        DISTRICT_COORDS = {
            "21010": (35.106, 129.032), "21020": (35.097, 129.024), "21030": (35.129, 129.045), # Joong, Seo, Dong
            "21040": (35.091, 129.068), "21050": (35.163, 129.053), "21060": (35.204, 129.083), # Yeongdo, Jin, Dongrae
            "21070": (35.136, 129.084), "21080": (35.197, 128.990), "21090": (35.163, 129.163), # Nam, Buk, Haeundae
            "21100": (35.104, 128.975), "21110": (35.243, 129.092), "21120": (35.212, 128.980), # Saha, Geumjeong, Gangseo
            "21130": (35.176, 129.079), "21140": (35.145, 129.113), "21150": (35.152, 128.991), # Yeonje, Suyeong, Sasang
            "21310": (35.244, 129.222)  # Gijang
        }

        FACILITY_TYPES = ['êµí†µ', 'ê³µê³µê³µê°„', 'ìœ„ìƒ', 'ë¬¸í™”', 'ì‚°ì—…', 'ì•ˆì „']

        for year_idx, year in enumerate(YEARS):
            print(f"Generating data for year {year}...")
            
            # Trend modifier: 2026 is baseline, 2025 is slightly lower, 2024 lower
            # To simulate improvement over time
            trend_mod = year_idx * -2.5 # 0 for 2026, -2.5 for 2025, -5.0 for 2024
            
            for dist in BUSAN_DISTRICTS:
                d_code = dist['id']
                d_center = DISTRICT_COORDS.get(d_code, (35.179, 129.075)) # Default Busan Center
                
                # 1. District Analysis
                # If real data exists for 2026 (index 0), use it. Else generate.
                # For 2025/2024, adjust the 2026 score or generate new.
                
                existing_scores = analysis_data.get(d_code, {})
                
                def get_score_for_year(key):
                    base = existing_scores.get(key, random.uniform(65, 85))
                    # Add meaningful jitter + trend
                    # Older years have slightly lower scores to show progress
                    val = base + trend_mod + random.uniform(-2, 2)
                    return max(0, min(100, val))

                analysis = models.DistrictAnalysis(
                    district_code=d_code,
                    year=year,
                    housing_score=get_score_for_year('housing_score'),
                    env_score=get_score_for_year('env_score'),
                    transport_score=get_score_for_year('transport_score'),
                    safety_score=get_score_for_year('safety_score'),
                    culture_score=get_score_for_year('culture_score'),
                    industry_score=get_score_for_year('industry_score'),
                    welfare_score=get_score_for_year('welfare_score'),
                    education_score=get_score_for_year('education_score')
                )
                db.add(analysis)

                # 2. Dummy Personas (5-8 per district per year)
                for i in range(random.randint(5, 8)):
                    p_age = random.choice([20, 30, 40, 50, 60, 70])
                    p_name = f"{dist['name']} ì‹œë¯¼ {i+1}"
                    
                    dummy_persona = models.Persona(
                        name=p_name,
                        age=p_age,
                        job=random.choice(['ìì˜ì—…ì', 'íšŒì‚¬ì›', 'í•™ìƒ', 'ì£¼ë¶€', 'ì€í‡´ì', 'í”„ë¦¬ëœì„œ', 'êµìœ¡ì']),
                        district_code=d_code,
                        year=year,
                        image_emoji=random.choice(['ğŸ‘©', 'ğŸ‘¨', 'ğŸ§‘', 'ğŸ‘µ', 'ğŸ‘´', 'ğŸ‘±â€â™€ï¸', 'ğŸ‘±']),
                        tags=[random.choice(FACILITY_TYPES) for _ in range(random.randint(1, 3))],
                        quote=f"{year}ë…„ {dist['name']}ì˜ ë³€í™”ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.",
                        full_quote=f"ì €ëŠ” {dist['name']}ì— {random.randint(1,20)}ë…„ì§¸ ê±°ì£¼ ì¤‘ì…ë‹ˆë‹¤. {year}ë…„ì—ëŠ” íŠ¹íˆ ë³´í–‰ í™˜ê²½ê³¼ ì•¼ê°„ ì•ˆì „ì´ ê°œì„ ë˜ê¸°ë¥¼ í¬ë§í•©ë‹ˆë‹¤.",
                        pain_points=[f"{random.choice(['ê°€ë¡œë“±', 'ë³´ë„ë¸”ë¡', 'CCTV', 'ê³µì›', 'ë²„ìŠ¤ì •ë¥˜ì¥'])} ê°œì„  í•„ìš”", "ì“°ë ˆê¸° íˆ¬ê¸° ë¬¸ì œ"],
                        suggestions=["ì•ˆì‹¬ ê·€ê°“ê¸¸ ì¡°ì„±", "ë¬¸í™” ì‹œì„¤ í™•ì¶©"],
                        expected_effects=["ì‚¶ì˜ ì§ˆ í–¥ìƒ", "ì•ˆì „í•œ ë§ˆì„ ë§Œë“¤ê¸°"],
                        stats={"safety_score": int(random.uniform(50, 90)), "satisfaction": int(random.uniform(50, 90))}
                    )
                    db.add(dummy_persona)

                # 3. Dummy Insights / Map Pins (To populate map for all years)
                # Generate 3-5 random insights per district per year
                for i in range(random.randint(3, 5)):
                    # Jitter coordinates around center
                    lat = d_center[0] + random.uniform(-0.015, 0.015)
                    lng = d_center[1] + random.uniform(-0.015, 0.015)
                    
                    cat_type = random.choice(FACILITY_TYPES)
                    source = random.choice(['survey', 'diagnosis'])
                    is_high = random.random() < 0.3 # 30% chance of High severity
                    
                    title = f"[{cat_type}] {dist['name']} {random.choice(['ë³´í–‰ ë¶ˆí¸', 'ì‹œì„¤ ë…¸í›„', 'ì•ˆì „ ìš°ë ¤', 'ìœ„ìƒ ë¬¸ì œ', 'ì¡°ëª… ë¶€ì¡±'])}"
                    
                    dummy_insight = models.DistrictInsight(
                        district_code=d_code,
                        year=year,
                        type='issue',
                        title=title,
                        description=f"{year}ë…„ í˜„ì¥ ì ê²€ ê²°ê³¼, {cat_type} ê´€ë ¨ ê°œì„ ì´ ì‹œê¸‰í•©ë‹ˆë‹¤.",
                        image_url=None, # No image for dummy
                        severity='High' if is_high else 'Medium',
                        date=datetime.now().replace(year=int(year)).strftime("%Y-%m-%d"),
                        proposer=f"{'ì „ë¬¸ê°€' if source == 'diagnosis' else 'ì‹œë¯¼'} íŒ¨ë„",
                        latitude=lat,
                        longitude=lng,
                        category=source
                    )
                    db.add(dummy_insight)

        print(f"Created analysis, Personas, and Insights for {len(BUSAN_DISTRICTS)} districts across {len(YEARS)} years.")
        db.commit()
        print("Data Import Completed successfully.")

    except Exception as e:
        import traceback
        with open("import_error.log", "w", encoding="utf-8") as f:
             f.write(traceback.format_exc())
             f.write(f"\nError importing data: {e}")
        print(f"Error importing data: {e}")
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    import_data()
